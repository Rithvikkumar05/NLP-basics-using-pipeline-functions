{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsVpTqHs-Zal",
        "outputId": "8e33f0f1-1a7f-49b1-b201-6147481d0248"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.16.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n",
            "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.16.0 textstat-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alPTRHKI5MOW",
        "outputId": "fd7fd51f-1e06-4eab-a076-8d7c81247123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis complete. Results saved to output.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textstat import textstat\n",
        "import re\n",
        "\n",
        "# Download required nltk resources\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to calculate average sentence length and complex words\n",
        "def text_statistics(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Average sentence length\n",
        "    avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
        "\n",
        "    # Complex words are words with 3 or more syllables\n",
        "    complex_words = [word for word in words if textstat.syllable_count(word) >= 3]\n",
        "    percentage_of_complex_words = len(complex_words) / len(words) * 100 if words else 0\n",
        "\n",
        "    return avg_sentence_length, percentage_of_complex_words, len(complex_words), len(words)\n",
        "\n",
        "# Function to extract article text\n",
        "def extract_article_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        title = soup.find('h1').get_text(strip=True)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "        return title, article_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to calculate sentiment scores using VADER\n",
        "def sentiment_analysis(text):\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "    positive_score = sentiment_scores['pos']\n",
        "    negative_score = sentiment_scores['neg']\n",
        "    polarity_score = sentiment_scores['compound']\n",
        "    return positive_score, negative_score, polarity_score\n",
        "\n",
        "# Function to calculate subjectivity score using TextBlob (an alternative to manual method)\n",
        "from textblob import TextBlob\n",
        "\n",
        "def subjectivity_score(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.subjectivity\n",
        "\n",
        "# Function to calculate Fog Index\n",
        "def fog_index(text):\n",
        "    return textstat.gunning_fog(text)\n",
        "\n",
        "# Function to count personal pronouns\n",
        "def count_personal_pronouns(text):\n",
        "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
        "    return len(pronouns)\n",
        "\n",
        "# Load the Excel file containing URLs\n",
        "input_file = 'Input.xlsx'\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Create a DataFrame for output results\n",
        "output_columns = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
        "                  'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
        "                  'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
        "                  'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "output_df = pd.DataFrame(columns=output_columns)\n",
        "\n",
        "# Initialize an empty list to collect the rows before creating a DataFrame\n",
        "rows = []\n",
        "\n",
        "# Loop through the URLs and extract data\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    title, article_text = extract_article_text(url)\n",
        "\n",
        "    if title and article_text:\n",
        "        # Perform sentiment analysis\n",
        "        positive_score, negative_score, polarity_score = sentiment_analysis(article_text)\n",
        "\n",
        "        # Calculate subjectivity score\n",
        "        subj_score = subjectivity_score(article_text)\n",
        "\n",
        "        # Calculate text metrics manually\n",
        "        avg_sentence_length, percent_complex_words, complex_word_count, word_count = text_statistics(article_text)\n",
        "        fog_idx = fog_index(article_text)  # Updated to pass the article text to the function\n",
        "        syllable_per_word = textstat.syllable_count(article_text) / word_count if word_count else 0\n",
        "        personal_pronouns = count_personal_pronouns(article_text)\n",
        "        avg_word_length = sum(len(word) for word in nltk.word_tokenize(article_text)) / word_count if word_count else 0\n",
        "\n",
        "        # Collect the results as a dictionary and add it to the list\n",
        "        rows.append({\n",
        "            'URL_ID': url_id, 'URL': url, 'POSITIVE SCORE': positive_score,\n",
        "            'NEGATIVE SCORE': negative_score, 'POLARITY SCORE': polarity_score,\n",
        "            'SUBJECTIVITY SCORE': subj_score, 'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "            'PERCENTAGE OF COMPLEX WORDS': percent_complex_words, 'FOG INDEX': fog_idx,\n",
        "            'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,\n",
        "            'COMPLEX WORD COUNT': complex_word_count, 'WORD COUNT': word_count,\n",
        "            'SYLLABLE PER WORD': syllable_per_word, 'PERSONAL PRONOUNS': personal_pronouns,\n",
        "            'AVG WORD LENGTH': avg_word_length\n",
        "        })\n",
        "\n",
        "# Once all rows are collected, create a DataFrame\n",
        "output_df = pd.DataFrame(rows)\n",
        "\n",
        "# Save the output to an Excel file\n",
        "output_file = 'output.xlsx'\n",
        "output_df.to_excel(output_file, index=False)\n",
        "print(f\"Analysis complete. Results saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24GHp13j-KU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}